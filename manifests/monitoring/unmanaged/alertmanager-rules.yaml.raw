apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: prometheus-k8s-rules
  namespace: monitoring
spec:
  groups:
    - name: kube-apiserver.rules
      rules:
        - expr: |
            sum(rate(apiserver_request_duration_seconds_sum{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod)
            /
            sum(rate(apiserver_request_duration_seconds_count{subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod)
          record: cluster:apiserver_request_duration_seconds:mean5m
        - expr: |
            histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",subresource!="log",verb!~"LIST|WATCH|WATCHLIST|PROXY|CONNECT"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:apiserver_request_duration_seconds:histogram_quantile
    - name: kube-apiserver-error
      rules:
        - alert: ErrorBudgetBurn
          annotations:
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
          expr: |
            (
              status_class_5xx:apiserver_request_total:ratio_rate1h{job="apiserver"} > (14.4*0.010000)
              and
              status_class_5xx:apiserver_request_total:ratio_rate5m{job="apiserver"} > (14.4*0.010000)
            )
            or
            (
              status_class_5xx:apiserver_request_total:ratio_rate6h{job="apiserver"} > (6*0.010000)
              and
              status_class_5xx:apiserver_request_total:ratio_rate30m{job="apiserver"} > (6*0.010000)
            )
          labels:
            job: apiserver
            severity: critical
        - alert: ErrorBudgetBurn
          annotations:
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-errorbudgetburn
          expr: |
            (
              status_class_5xx:apiserver_request_total:ratio_rate1d{job="apiserver"} > (3*0.010000)
              and
              status_class_5xx:apiserver_request_total:ratio_rate2h{job="apiserver"} > (3*0.010000)
            )
            or
            (
              status_class_5xx:apiserver_request_total:ratio_rate3d{job="apiserver"} > (0.010000)
              and
              status_class_5xx:apiserver_request_total:ratio_rate6h{job="apiserver"} > (0.010000)
            )
          labels:
            job: apiserver
            severity: warning
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[5m]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate5m
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[30m]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate30m
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[1h]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate1h
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[2h]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate2h
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[6h]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate6h
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[1d]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate1d
        - expr: |
            sum by (status_class) (
              label_replace(
                rate(apiserver_request_total{job="apiserver"}[3d]
              ), "status_class", "${1}xx", "code", "([0-9])..")
            )
          labels:
            job: apiserver
          record: status_class:apiserver_request_total:rate3d
        - expr: |
            sum(status_class:apiserver_request_total:rate5m{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate5m{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate5m
        - expr: |
            sum(status_class:apiserver_request_total:rate30m{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate30m{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate30m
        - expr: |
            sum(status_class:apiserver_request_total:rate1h{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate1h{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate1h
        - expr: |
            sum(status_class:apiserver_request_total:rate2h{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate2h{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate2h
        - expr: |
            sum(status_class:apiserver_request_total:rate6h{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate6h{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate6h
        - expr: |
            sum(status_class:apiserver_request_total:rate1d{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate1d{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate1d
        - expr: |
            sum(status_class:apiserver_request_total:rate3d{job="apiserver",status_class="5xx"})
            /
            sum(status_class:apiserver_request_total:rate3d{job="apiserver"})
          labels:
            job: apiserver
          record: status_class_5xx:apiserver_request_total:ratio_rate3d
    - name: kubernetes-system-apiserver
      rules:
        - alert: KubeAPILatencyHigh
          annotations:
            message:
              The API server has an abnormal latency of {{ $value }} seconds for
              {{ $labels.verb }} {{ $labels.resource }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
          expr: |
            (
              cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"}
              >
              on (verb) group_left()
              (
                avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
                +
                2*stddev by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
              )
            ) > on (verb) group_left()
            1.2 * avg by (verb) (cluster:apiserver_request_duration_seconds:mean5m{job="apiserver"} >= 0)
            and on (verb,resource)
            cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"}
            >
            1
          for: 5m
          labels:
            severity: warning
        - alert: KubeAPILatencyHigh
          annotations:
            message:
              The API server has a 99th percentile latency of {{ $value }} seconds
              for {{ $labels.verb }} {{ $labels.resource }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapilatencyhigh
          expr: |
            cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99"} > 4
          for: 10m
          labels:
            severity: critical
        - alert: KubeAPIErrorsHigh
          annotations:
            message:
              API server is returning errors for {{ $value | humanizePercentage
              }} of requests.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
          expr: |
            sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
              /
            sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.03
          for: 10m
          labels:
            severity: critical
        - alert: KubeAPIErrorsHigh
          annotations:
            message:
              API server is returning errors for {{ $value | humanizePercentage
              }} of requests.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
          expr: |
            sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m]))
              /
            sum(rate(apiserver_request_total{job="apiserver"}[5m])) > 0.01
          for: 10m
          labels:
            severity: warning
        - alert: KubeAPIErrorsHigh
          annotations:
            message:
              API server is returning errors for {{ $value | humanizePercentage
              }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource
              }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
          expr: |
            sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
              /
            sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) > 0.10
          for: 10m
          labels:
            severity: critical
        - alert: KubeAPIErrorsHigh
          annotations:
            message:
              API server is returning errors for {{ $value | humanizePercentage
              }} of requests for {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource
              }}.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapierrorshigh
          expr: |
            sum(rate(apiserver_request_total{job="apiserver",code=~"5.."}[5m])) by (resource,subresource,verb)
              /
            sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) > 0.05
          for: 10m
          labels:
            severity: warning
        - alert: KubeClientCertificateExpiration
          annotations:
            message:
              A client certificate used to authenticate to the apiserver is expiring
              in less than 7.0 days.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
          expr: |
            apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
          labels:
            severity: warning
        - alert: KubeClientCertificateExpiration
          annotations:
            message:
              A client certificate used to authenticate to the apiserver is expiring
              in less than 24.0 hours.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeclientcertificateexpiration
          expr: |
            apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
          labels:
            severity: critical
        - alert: KubeAPIDown
          annotations:
            message: KubeAPI has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeapidown
          expr: |
            absent(up{job="apiserver"} == 1)
          for: 15m
          labels:
            severity: critical
    - name: kubernetes-system-kubelet
      rules:
        - alert: KubeNodeNotReady
          annotations:
            message: "{{ $labels.node }} has been unready for more than 15 minutes."
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodenotready
          expr: |
            kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
          for: 15m
          labels:
            severity: warning
        - alert: KubeNodeUnreachable
          annotations:
            message: "{{ $labels.node }} is unreachable and some workloads may be rescheduled."
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubenodeunreachable
          expr: |
            kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} == 1
          labels:
            severity: warning
        - alert: KubeletTooManyPods
          annotations:
            message:
              Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
              }} of its Pod capacity.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubelettoomanypods
          expr: |
            max(max(kubelet_running_pod_count{job="kubelet"}) by(instance) * on(instance) group_left(node) kubelet_node_name{job="kubelet"}) by(node) / max(kube_node_status_capacity_pods{job="kube-state-metrics"}) by(node) > 0.95
          for: 15m
          labels:
            severity: warning
        - alert: KubeletDown
          annotations:
            message: Kubelet has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeletdown
          expr: |
            absent(up{job="kubelet"} == 1)
          for: 15m
          labels:
            severity: critical
    - name: kubernetes-system-scheduler
      rules:
        - alert: KubeSchedulerDown
          annotations:
            message: KubeScheduler has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubeschedulerdown
          expr: |
            absent(up{job="kube-scheduler"} == 1)
          for: 15m
          labels:
            severity: critical
    - name: kube-scheduler.rules
      rules:
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.99"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.9"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
        - expr: |
            histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
          labels:
            quantile: "0.5"
          record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: kubernetes-system-controller-manager
      rules:
        - alert: KubeControllerManagerDown
          annotations:
            message: KubeControllerManager has disappeared from Prometheus target discovery.
            runbook_url: https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubecontrollermanagerdown
          expr: |
            absent(up{job="kube-controller-manager"} == 1)
          for: 15m
          labels:
            severity: critical
    - name: etcd
      rules:
        - alert: etcdInsufficientMembers
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": insufficient members ({{ $value
              }}).'
          expr: |
            sum(up{job=~".*etcd.*"} == bool 1) by (job) < ((count(up{job=~".*etcd.*"}) by (job) + 1) / 2)
          for: 3m
          labels:
            severity: critical
        - alert: etcdNoLeader
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has
              no leader.'
          expr: |
            etcd_server_has_leader{job=~".*etcd.*"} == 0
          for: 1m
          labels:
            severity: critical
        - alert: etcdHighNumberOfLeaderChanges
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": instance {{ $labels.instance }}
              has seen {{ $value }} leader changes within the last hour.'
          expr: |
            rate(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}[15m]) > 3
          for: 15m
          labels:
            severity: warning

        # grpc_service!="etcdserverpb.Watch" added as a workaround to https://bugzilla.redhat.com/show_bug.cgi?id=1677689
        - alert: etcdHighNumberOfFailedGRPCRequests
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
              $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
          expr: |
            100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK",grpc_service!="etcdserverpb.Watch"}[5m])) BY (job, instance, grpc_service, grpc_method)
              /
            sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
              > 1
          for: 10m
          labels:
            severity: warning
        - alert: etcdHighNumberOfFailedGRPCRequests
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": {{ $value }}% of requests for {{
              $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
          expr: |
            100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code!="OK",grpc_service!="etcdserverpb.Watch"}[5m])) BY (job, instance, grpc_service, grpc_method)
              /
            sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) BY (job, instance, grpc_service, grpc_method)
              > 5
          for: 5m
          labels:
            severity: critical
        - alert: etcdGRPCRequestsSlow
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": gRPC requests to {{ $labels.grpc_method
              }} are taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
          expr: |
            histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_type="unary"}[5m])) by (job, instance, grpc_service, grpc_method, le))
            > 0.15
          for: 10m
          labels:
            severity: critical
        - alert: etcdMemberCommunicationSlow
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To
              }} is taking {{ $value }}s on etcd instance {{ $labels.instance }}.'
          expr: |
            histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
            > 0.15
          for: 10m
          labels:
            severity: warning
        - alert: etcdHighNumberOfFailedProposals
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": {{ $value }} proposal failures within
              the last hour on etcd instance {{ $labels.instance }}.'
          expr: |
            rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
          for: 15m
          labels:
            severity: warning
        - alert: etcdHighFsyncDurations
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": 99th percentile fync durations are
              {{ $value }}s on etcd instance {{ $labels.instance }}.'
          expr: |
            histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
            > 0.5
          for: 10m
          labels:
            severity: warning
        - alert: etcdHighCommitDurations
          annotations:
            message:
              'etcd cluster "{{ $labels.job }}": 99th percentile commit durations
              {{ $value }}s on etcd instance {{ $labels.instance }}.'
          expr: |
            histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
            > 0.25
          for: 10m
          labels:
            severity: warning
        - alert: etcdHighNumberOfFailedHTTPRequests
          annotations:
            message:
              "{{ $value }}% of requests for {{ $labels.method }} failed on etcd
              instance {{ $labels.instance }}"
          expr: |
            sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
            BY (method) > 0.01
          for: 10m
          labels:
            severity: warning
        - alert: etcdHighNumberOfFailedHTTPRequests
          annotations:
            message:
              "{{ $value }}% of requests for {{ $labels.method }} failed on etcd
              instance {{ $labels.instance }}."
          expr: |
            sum(rate(etcd_http_failed_total{job=~".*etcd.*", code!="404"}[5m])) BY (method) / sum(rate(etcd_http_received_total{job=~".*etcd.*"}[5m]))
            BY (method) > 0.05
          for: 10m
          labels:
            severity: critical
        - alert: etcdHTTPRequestsSlow
          annotations:
            message:
              etcd instance {{ $labels.instance }} HTTP requests to {{ $labels.method
              }} are slow.
          expr: |
            histogram_quantile(0.99, rate(etcd_http_successful_duration_seconds_bucket[5m]))
            > 0.15
          for: 10m
          labels:
            severity: warning
